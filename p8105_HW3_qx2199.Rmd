---
title: "p8105_hw3_qx2199"
author: "Qianhui Xu"
date: '2020-10-08'
output: github_document
---


```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Load the Instacart data.

```{r}
data("instacart")
```

The Instacart dataset contains `r nrow(instacart)` rows 
and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- product ID, product name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```
As we can see, there are 134 aisles, and  " fresh vegetables "are which the most items from.

Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize the plot.

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

The top 2 aisles are "fresh vegetables" and "fresh fruits".

Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in the table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

The top 3 items in baking ingredients aisle are "Light Brown Sugar", "Pure Baking Soda", and "Cane Sugar".

The top 3 items in dog food care aisle are "Snack Sticks Chicken & Rice Recipe Dog Treats", "Organix Chicken & Brown Rice Recipe", and "Small Dog Biscuits".

The top 3 items in packaged vegetables fruits aisle are "Organic Baby Spinach", "Organic Raspberries", and "Organic Blueberries".

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format the table as a 2 x 7 table.

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```


## Problem 2

Load, tidy, and otherwise wrangle the accelerometers data

```{r}
accel_df = read.csv('./accel_data.csv') %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity.",
    values_to = "activity_counts"
  ) %>% 
mutate(day = factor (day, levels = c ("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")))%>%

  mutate(day_type = case_when(
      day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
      day %in% c("Saturday", "Sunday")~ "weekend")) %>%

  mutate(minute = as.numeric(minute)) %>%
arrange(week,day)
```
data discription:
The tidy accelerometers dataset include `r ncol(accel_df)` variables and `r nrow(accel_df)` rows.The variable names are 'week', 'day','minute','activity counts' 'day id' and 'day type'.


Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals.

```{r}
accel_df_new = accel_df %>% 
  group_by(week, day) %>% 
  summarise(total_activity_per_day = sum(activity_counts)) %>% 
  arrange(week, day) %>% 
pivot_wider(
  names_from = day,
  values_from = total_activity_per_day
)  
```
The person tends to have a relative stable activities during the weekdays, especially on Tuesdays and Wednesdays.Also, the person has relatively more activities on Fridays. 

Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.


```{r}

accel_df %>% 
    mutate(
    hour = ceiling(minute/60)
  ) %>% 
  group_by (hour, day_id) %>% 
  mutate(mean_activity_counts = mean(activity_counts)) %>% 
  ggplot(aes(x = hour, y = mean_activity_counts, colour = day, group = day_id)) + geom_point(alpha= 0.3) +geom_line(alpha=0.5)+
    labs( title = "Activity time for each day ",
      x = "24-Hours activity time",
      y = "Daily activity counts"
    )

```
From the graph we could see that the person have relatively more activities on Fridays and more 
activities on the evening of Friday. On Sundays the person are more likely to take exercise from 10am to 1 pm.
Also, the the peak for activities are mainly from 6am to 1pm,and from 7pm to 10 pm. The person has relativly less activity at around 3 pm.

##Problem 3

Do data cleaning. Create separate variables for year, month, and day.

```{r}
data("ny_noaa")
noaa_df = ny_noaa %>% 
janitor::clean_names() %>%  
separate(date, into = c("year", "month", "day"), sep = "-", convert = TRUE) %>% 
mutate(year = as.integer(year),
     month = as.integer(month),
     day = as.integer(day)) %>%
mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin)
         ) %>%
mutate(month = month.name[month]) %>%
mutate(tmax = tmax/10,tmin = tmin/10,prcp = prcp/10) 
   
```
Discription of the dataset: The `noaa_df` dataset has  `r nrow(noaa_df)` rows and `r ncol(noaa_df)` columns. 
The variable names are `id`for  weather station ID , `year` for observation year, `month` for observation month  `day`for observation day, `prcp`for precipitation（the value needs to be devided by ten when transfer to mm scale),`snow`for snowfall in mm,`snwd`for snow depth in mm,`tmax`for maximum temperature (the value needs to be devided by ten in Celsius scale), and`tmin`for minimum temperature (the value needs to be devided by ten in Celsius scale)

For snowfall, the most commonly observed values are:

```{r}
noaa_df %>% 
count(snow) %>%
arrange(desc(n))
```
 As we can see, the the most commonly observed values for snowfall are 0.I think this is because in the most time of the year, there are no snowfalls. 
 
Make a two-panel plot showing the average max temperature in January and in July in each station across years. 

```{r}
noaa_df %>% 
  filter(month == c("January", "July")) %>% 
  group_by(id, year, month) %>%
  mutate(
    mean_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_tmax, color = month)) + 
  geom_point(alpha = 0.3) +  geom_line(alpha = 0.5) +
  facet_grid(~month) +
  theme(legend.position="none") +
  scale_color_manual(values=c("blue", "green")) + labs(
    title = "Average max temperature in January and July across years",
    x = "Year",
    y = "Average max temperature (C)"
    ) 
```

As we can see, the average max temperature in Janurary is lower than that in July , and the temperature in both months fluctuates each year. 
In January the max average tempreature are mainly fluctuate  around 0 (Celsius), in July the max average tempreature are mainly fluctuate  around 27 (Celsius).
The max mean temperatures in Januraries are more fluctuable than the max mean temperature in Julys, 
There are outlier in the two graphs. In January, there is a high outlier which is about 13 (Celsius). In July, there are two low outliers, one is around 13(Celsius) and another is around 17(Celsius).

Make a two-panel plot showing (i) tmax vs tmin for the full dataset 
```{r}
tmax_vs_tmin =
  noaa_df %>% 
  ggplot(aes(x = tmin, y = tmax))+
    geom_smooth(se = FALSE) +
    labs(
      title = "tmax vs tmin plot",
      x = "Minimum daily temperature (C)",
      y = "Maximum daily temperature (C)" )

snow_distri  =
  noaa_df %>% 
  filter(snow < 100 , snow > 0) %>%
  mutate(year = as.factor(year)) %>%
  ggplot(aes(x = year, y = snow)) + 
  geom_violin(aes(fill = year), alpha = 0.3 )+
  labs(
      title = "Distribution of the snowfall plot",
      x = "Year",
      y = "Snowfall"
    )+
  theme(axis.text.x = element_text(angle = 90), legend.position="none")

tmax_vs_tmin / snow_distri

```













